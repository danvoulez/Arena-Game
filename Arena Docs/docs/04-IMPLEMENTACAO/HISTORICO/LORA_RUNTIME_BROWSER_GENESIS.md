# LoRA Runtime Browser: Genesis Span
## Treinamento Comput√°vel de LoRA no Navegador

**Autor:** Dan Voulez & AI Architect  
**Data:** 2025-11-09  
**Vers√£o:** 0.1-alpha  
**Status:** Proposed

---

## üìã Genesis Span Original

```json
{
  "who": "Dan Voulez & AI Architect",
  "did": "ARCHITECT_COMPUTABLE_LORA_RUNTIME",
  "this": "Genesis of LoRA Training via JSON‚úØAtomic inside Browser Environment",
  "when": "2025-11-09T23:59:00Z",
  "status": "proposed",
  "confirmed_by": "design_alignment_dan_voulez_2025-11-09",
  "if_ok": "BEGIN_IMPLEMENTATION_STEPS(1-4)",
  "if_not": "FALLBACK_TO_EXTERNAL_LORA_AGENT_WITH_TRACE_SIGNATURE",
  "metadata": {
    "version": "0.1-alpha",
    "vision": "A computable LoRA runtime, running 100% in-browser, using WebGPU + WASM + JSON‚úØAtomic spans to log, verify and evolve language model agents as 'creatures'.",
    "philosophy": "The act of training an AI should be self-contained, verifiable, human-readable and ownable. If a LoRA patch cannot be exported, signed and reapplied, then it n√£o √© do Arena.",
    "pipeline_design": {
      "runtime": "JavaScript-first, Executable via WebGPU or WASM",
      "formato_dos_pesos": "TypedArray / Blob exportable patch (.jsonl or .bin)",
      "logica_de_treino": "Few-shot LoRA micro-fine-tuning com cache de gradiente sob controle de spans",
      "criptografia": "Hash BLAKE3 + Assinatura Ed25519 para cada patch + metadata",
      "modelo_base": "GPT2, DistilGPT, TinyLLaMA ou Mistral 7B quantizado (GQA-friendly)"
    },
    "phases": [
      {
        "phase": 1,
        "title": "Runtime Esquel√©tico Execut√°vel",
        "objetivo": "Criar a funda√ß√£o de execu√ß√£o local via WebGPU + fallback WASM puro (para fallback em iPhone, por exemplo).",
        "artefato_chave": "src/lora-runtime-browser.ts + src/kernel-wasm/train_lora.wasm"
      },
      {
        "phase": 2,
        "title": "Treino com Dataset Local e Span At√¥mico",
        "objetivo": "Permitir que o jogador envie um `.jsonl` com exemplos e aplique treino local leve, com span assinado.",
        "dataset_sample": "dataset_hash + examples_tokens + config_lora",
        "span_gerado": {
          "type": "training",
          "fields": [
            "creature_id",
            "dataset_hash",
            "duration_ms",
            "model_checkpoint",
            "patch_blake3",
            "signature_ed25519",
            "prompt_before_after"
          ]
        }
      },
      {
        "phase": 3,
        "title": "Exporta√ß√£o e Reuso de Patch",
        "objetivo": "Permitir exporta√ß√£o do patch como Blob (file download) e reimporta√ß√£o com verifica√ß√£o de assinatura.",
        "export_format": ".lora_patch.jsonl",
        "verifier_script": "src/verify-lora-patch.ts"
      },
      {
        "phase": 4,
        "title": "Evolu√ß√£o e Ascens√£o Baseada em LoRA",
        "objetivo": "Somente criaturas com patch LoRA assinado podem ascender. A LoRA √© o ritual de transi√ß√£o.",
        "check": "validate(lora_patch.signature + dna.history) ‚Üí allow_ascend()"
      }
    ],
    "target_platforms": [
      "macOS Safari (M1/M2+)",
      "Chrome (desktop/mobile)",
      "iPhone (via WASM-only fallback)",
      "PWA exclusive"
    ],
    "tech_notes": {
      "max_model_size": "~300MB por inst√¢ncia carregada",
      "requisitos_ram": "2-4 GB livres para patch leve",
      "compress√£o_opcional": "support GZIP or Brotli",
      "webgpu_fallback": "wasm + int8 matrix ops",
      "authenticity": "Ed25519 key injected via DV25-Seal for creature + trainer"
    }
  },
  "hash": "blake3:0f12e78a9b3c654facc913a37f20a44d4431aa00129243bce8dcfe22f7af3139",
  "signature": "ed25519:54ac29ef8fa2398dcfb012e9423a7c4eb71a5fe33c980b38bcdb20a401a1e54ac29ef8fa2398dcfb012e9423a7c4eb"
}
```

---

## üéØ Vis√£o e Filosofia

### Vis√£o

Um runtime de LoRA comput√°vel, rodando **100% no navegador**, usando WebGPU + WASM + JSON‚úØAtomic spans para registrar, verificar e evoluir agentes de linguagem como "criaturas".

### Filosofia

> **"O ato de treinar uma IA deve ser autocontido, verific√°vel, leg√≠vel por humanos e possu√≠vel. Se um patch LoRA n√£o pode ser exportado, assinado e reaplicado, ent√£o ele n√£o √© do Arena."**

**Princ√≠pios:**
1. **Self-contained:** Tudo roda no browser, sem depend√™ncias externas
2. **Verifiable:** Cada patch tem hash BLAKE3 + assinatura Ed25519
3. **Human-readable:** Spans JSON‚úØAtomic documentam todo o processo
4. **Ownable:** Patch export√°vel como arquivo, reimport√°vel com verifica√ß√£o

---

## üèóÔ∏è Pipeline Design

### Runtime

- **JavaScript-first:** TypeScript/Deno como linguagem principal
- **WebGPU:** Acelera√ß√£o via GPU quando dispon√≠vel
- **WASM Fallback:** Para dispositivos sem WebGPU (iPhone, etc.)

### Formato dos Pesos

- **TypedArray / Blob:** Patch export√°vel
- **Formatos:** `.jsonl` (metadados + pesos) ou `.bin` (bin√°rio puro)
- **Compress√£o:** GZIP ou Brotli opcional

### L√≥gica de Treino

- **Few-shot LoRA:** Micro fine-tuning com poucos exemplos
- **Cache de gradiente:** Sob controle de spans
- **Quantiza√ß√£o:** Suporte a int8 para modelos menores

### Criptografia

- **Hash BLAKE3:** Para integridade do patch
- **Assinatura Ed25519:** Para autenticidade (DV25-Seal)
- **Metadata assinada:** Configura√ß√£o, dataset hash, etc.

### Modelo Base

- **Suportados:** GPT-2, DistilGPT, TinyLLaMA, Mistral 7B quantizado
- **GQA-friendly:** Grouped Query Attention para efici√™ncia
- **Tamanho m√°ximo:** ~300MB por inst√¢ncia carregada

---

## üìÖ Fases de Implementa√ß√£o

### Fase 1: Runtime Esquel√©tico Execut√°vel

**Objetivo:** Criar a funda√ß√£o de execu√ß√£o local via WebGPU + fallback WASM puro.

**Artefatos:**
- `src/lora-runtime-browser.ts` - Runtime principal
- `src/kernel-wasm/train_lora.wasm` - Kernel WASM para treino
- `src/webgpu-kernel.ts` - Kernel WebGPU (quando dispon√≠vel)

**Checklist:**
- [ ] WebGPU detection e fallback para WASM
- [ ] Carregamento de modelo base (GPT-2 quantizado)
- [ ] Kernel de treino b√°sico funcional
- [ ] Teste: Treino de 1 exemplo funciona

**Estimativa:** 5-7 dias

---

### Fase 2: Treino com Dataset Local e Span At√¥mico

**Objetivo:** Permitir que o jogador envie um `.jsonl` com exemplos e aplique treino local leve, com span assinado.

**Dataset Sample:**
```json
{
  "dataset_hash": "blake3:...",
  "examples_tokens": [[1, 2, 3, ...], ...],
  "config_lora": {
    "rank": 4,
    "alpha": 16,
    "dropout": 0.1
  }
}
```

**Span Gerado:**
```json
{
  "type": "training",
  "who": "creature_lyria_001",
  "did": "TRAIN_LORA_PATCH",
  "this": {
    "creature_id": "lyria_001",
    "dataset_hash": "blake3:...",
    "duration_ms": 45000,
    "model_checkpoint": "gpt2-base-quantized",
    "patch_blake3": "blake3:...",
    "signature_ed25519": "ed25519:...",
    "prompt_before_after": {
      "before": "Hello, how are you?",
      "after": "Hello! I'm doing great, thanks for asking!"
    }
  },
  "when": "2025-11-10T10:00:00Z",
  "status": "completed"
}
```

**Checklist:**
- [ ] Upload de dataset `.jsonl` funcional
- [ ] Tokeniza√ß√£o de exemplos
- [ ] Treino LoRA com configura√ß√£o
- [ ] Gera√ß√£o de span `training` assinado
- [ ] Teste: Treino completo gera patch + span

**Estimativa:** 4-5 dias

---

### Fase 3: Exporta√ß√£o e Reuso de Patch

**Objetivo:** Permitir exporta√ß√£o do patch como Blob (file download) e reimporta√ß√£o com verifica√ß√£o de assinatura.

**Export Format:**
```jsonl
{"type": "lora_patch_manifest", "version": "1.0", "creature_id": "...", ...}
{"type": "lora_patch_weights", "data": "base64:...", ...}
{"type": "lora_patch_signature", "hash": "blake3:...", "signature": "ed25519:..."}
```

**Verifier Script:**
```typescript
// src/verify-lora-patch.ts
async function verifyLoraPatch(patch: Blob): Promise<boolean> {
  // 1. Parse .lora_patch.jsonl
  // 2. Verificar hash BLAKE3
  // 3. Verificar assinatura Ed25519
  // 4. Validar metadata
  return isValid
}
```

**Checklist:**
- [ ] Export de patch como `.lora_patch.jsonl`
- [ ] Download de arquivo funcional
- [ ] Import de patch com verifica√ß√£o
- [ ] `verify-lora-patch.ts` funcional
- [ ] Teste: Export ‚Üí Import ‚Üí Verifica√ß√£o passa

**Estimativa:** 3-4 dias

---

### Fase 4: Evolu√ß√£o e Ascens√£o Baseada em LoRA

**Objetivo:** Somente criaturas com patch LoRA assinado podem ascender. A LoRA √© o ritual de transi√ß√£o.

**Check de Elegibilidade:**
```typescript
function canAscend(creature: Creature): boolean {
  const loraPatch = creature.loraPatches.find(p => p.isValid)
  const dnaHistory = creature.dnaHistory
  
  // Validar patch LoRA
  if (!loraPatch || !verifyLoraPatch(loraPatch)) {
    return false
  }
  
  // Validar DNA history
  if (!validateDnaHistory(dnaHistory)) {
    return false
  }
  
  // Combinar assinaturas
  const combinedSignature = combineSignatures(
    loraPatch.signature,
    dnaHistory.merkleRoot
  )
  
  return verifyCombinedSignature(combinedSignature)
}
```

**Checklist:**
- [ ] Valida√ß√£o de patch LoRA antes de ascens√£o
- [ ] Combina√ß√£o de assinaturas (patch + DNA)
- [ ] Endpoint `/api/arena/creatures/:id/ascend` verifica LoRA
- [ ] Teste: Criatura sem LoRA n√£o pode ascender
- [ ] Teste: Criatura com LoRA v√°lido ascende

**Estimativa:** 2-3 dias

---

## üéØ Plataformas Alvo

### macOS Safari (M1/M2+)

- **WebGPU:** ‚úÖ Suportado (Safari 18+)
- **WASM:** ‚úÖ Suportado
- **Performance:** Excelente (Metal acceleration)

### Chrome (Desktop/Mobile)

- **WebGPU:** ‚úÖ Suportado
- **WASM:** ‚úÖ Suportado
- **Performance:** Excelente

### iPhone (WASM-only Fallback)

- **WebGPU:** ‚ùå N√£o suportado (iOS 18+ pode ter)
- **WASM:** ‚úÖ Suportado
- **Performance:** Boa (CPU-only, mais lento)

### PWA Exclusive

- **Offline:** ‚úÖ Funciona offline
- **Cache:** Service Worker cache de modelos
- **Storage:** IndexedDB para patches

---

## üîß Notas T√©cnicas

### Limita√ß√µes

- **Max Model Size:** ~300MB por inst√¢ncia carregada
- **RAM Requirements:** 2-4 GB livres para patch leve
- **Training Time:** 30s - 5min dependendo do dispositivo

### Otimiza√ß√µes

- **Compress√£o:** GZIP ou Brotli para patches
- **Quantiza√ß√£o:** int8 para modelos menores
- **Cache:** Service Worker cache de modelos base
- **Lazy Loading:** Carregar modelo apenas quando necess√°rio

### Seguran√ßa

- **DV25-Seal:** Ed25519 key para creature + trainer
- **Verifica√ß√£o:** Hash BLAKE3 + assinatura em cada patch
- **Sandbox:** WASM roda em sandbox seguro

---

## üìä M√©tricas de Sucesso

### Fase 1: Runtime

- [ ] WebGPU detection funciona em 90%+ dos browsers modernos
- [ ] Fallback WASM funciona em 100% dos browsers
- [ ] Modelo base carrega em < 5s

### Fase 2: Treino

- [ ] Treino de 10 exemplos completa em < 2min (WebGPU) ou < 5min (WASM)
- [ ] 100% dos treinos geram span assinado
- [ ] Patch gerado √© v√°lido e verific√°vel

### Fase 3: Export/Import

- [ ] Export funciona em 100% dos casos
- [ ] Import com verifica√ß√£o funciona em 100% dos casos
- [ ] Patches inv√°lidos s√£o rejeitados

### Fase 4: Ascens√£o

- [ ] 100% das ascens√µes requerem LoRA v√°lido
- [ ] Verifica√ß√£o de assinatura funciona
- [ ] Criaturas sem LoRA s√£o bloqueadas

---

## üîó Integra√ß√£o com ArenaLab

### Fluxo Completo

```
1. Criatura atinge Level 15
   ‚Üì
2. Usu√°rio faz upload de dataset (.jsonl)
   ‚Üì
3. LoRA Runtime treina patch no browser
   ‚Üì
4. Span `training` gerado e assinado
   ‚Üì
5. Patch exportado como `.lora_patch.jsonl`
   ‚Üì
6. Patch aplicado √† criatura (evolu√ß√£o)
   ‚Üì
7. Span `evolution` gerado
   ‚Üì
8. Criatura pode ascender (com LoRA v√°lido)
```

### Endpoints Relacionados

- `POST /api/arena/creatures/:id/train` - Inicia treino LoRA
- `GET /api/arena/creatures/:id/training/status` - Status do treino
- `POST /api/arena/creatures/:id/export-lora` - Exporta patch
- `POST /api/arena/creatures/:id/import-lora` - Importa patch
- `POST /api/arena/creatures/:id/ascend` - Ascens√£o (requer LoRA)

---

## üöÄ Pr√≥ximos Passos

1. **Revisar este Genesis Span** com foco t√©cnico
2. **Validar viabilidade** de WebGPU/WASM no Mac mini
3. **Prototipar Fase 1** (Runtime esquel√©tico)
4. **Iterar** baseado em performance real

---

**"If a LoRA patch cannot be exported, signed and reapplied, then it n√£o √© do Arena."**

‚Äî Dan Voulez

---

*Documento gerado em: 2025-11-09*  
*Vers√£o: 0.1-alpha*  
*Status: Proposed*

